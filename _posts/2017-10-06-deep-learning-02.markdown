---
layout:     post
title:      "Coursera deep learning学习笔记"
subtitle:   "S02"
date:       2017-10-06
author:     "brucechen"
header-img: "img/post-bg-ai.jpg"
published: true
tags:
    - DL
    - 读书笔记
---

### 逻辑回归
线性回归的形式为`y=w^T*x+b`，当需要应用于分类问题时，需要在线性回归的基础上添加函数使得输出可以映射到0-1之间，常用的有`Sigmoid函数`，函数形式为`y(z) = 1/(1+e^-z)`。
代价函数`L=1/2*(y'-y)^2`，代价函数用来描述模型预测的结果与真实结果之间的误差。
上述代价函数虽然可以表示模型的误差，但是并不容易求取最优解，所以通常会使用另外一种代价函数`L=-(ylogy'+(1-y)log(1-y'))`

### 反向传播算法
链式求导，根据损失函数获得参数的偏导数，求得最优解。

### 梯度下降
通过梯度下降，可以逐步优化结果达到局部最优解，对于凸函数则是全局最优解。
`w = w - α(dj/dw)`

### 矩阵计算
对于多维向量的计算，通过多层for循环的计算通常是十分慢的，而numpy对于向量的计算做了很多优化，所以向量计算推荐使用numpy内置函数。